{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation using Transformers"
      ],
      "metadata": {
        "id": "LwGrBiVqu1Jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Libraries"
      ],
      "metadata": {
        "id": "ZhyUWkTcYw0p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "agBmyO40C8-u"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "Zb0qcQWeY0Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "oYx1pSqCU1iC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ],
      "metadata": {
        "id": "P9jGGzlnKoiZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading pre-trained Model & Tokenizer"
      ],
      "metadata": {
        "id": "_iFo1iEYY3EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained 'GPT-2' model and tokenizer\n",
        "model_name = \"gpt2-large\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "kVaHWiTtKrYh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Model & Tokenizer"
      ],
      "metadata": {
        "id": "ExTCKWcKZC1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode (no training)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuvajJKyKvx0",
        "outputId": "814bd6a4-90ad-4db8-d087-9854f5bb8f0e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1280)\n",
              "    (wpe): Embedding(1024, 1280)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-35): 36 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "U4SNHv9xXF-7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Text"
      ],
      "metadata": {
        "id": "yPtBNsqpY_US"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zffC4oJAElsC"
      },
      "outputs": [],
      "source": [
        "# Function to generate text\n",
        "def generate_text_from_model(prompt, max_length=50, temperature=0.7):\n",
        "\n",
        "    # Encoding prompt to token\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    # Decode and return generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text with a prompt\n",
        "def text_generation():\n",
        "\n",
        "  # Prompt input from user\n",
        "  prompt = str(input('Enter prompt: '))\n",
        "\n",
        "  # Generating text from model\n",
        "  generated_text = generate_text_from_model(prompt, max_length = 100)\n",
        "\n",
        "  # Printing generated text\n",
        "  print(generated_text)"
      ],
      "metadata": {
        "id": "K1jrVLHbzXtI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aScR2V8K8I9",
        "outputId": "089f81e2-eaa5-49a4-f79f-acde40dad138"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter prompt: I love my nation India\n",
            "I love my nation India. I love India's culture. But I also love the people of India, and I want to see them prosper. And I think that's what we're going to do. We're not going into a war with India.\"\n",
            "\n",
            "The Indian government has been trying to get the U.S. to stop selling arms to Pakistan, which is accused of sponsoring terrorism.\n",
            ".@POTUS: \"We're gonna work with the Indian people to make sure that they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWsQ8MUCatcW",
        "outputId": "6f4d3626-a90e-4ffb-bfe7-95f7932d927b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter prompt: I am a cricketer\n",
            "I am a cricketer, and I am not a cricket fan. I have never watched a Test match. But I do know that the Indian team is a very good team. They have a lot of talent.\n",
            "\n",
            "\"I have seen them play in the World Cup. It is very exciting. The Indian players are very talented. There are a few players who are not so good, but they are good enough to play for India. So I think they will be a good side.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_bp3otZbLVv",
        "outputId": "6bf3ebad-29a1-4f00-fcdc-aa2d3b0133c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter prompt: I love songs of One Direction\n",
            "I love songs of One Direction, but I don't think they're the best band in the world. I think the band is great, and I love the music, I just don\n",
            "\n",
            "I think that the song is a little bit too long. It's a bit of a stretch.\n",
            "The song's not bad, it's just a tad long, so I'm not sure if I'd want to listen to it. But I do think it would be a good song to play in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9miafoUbm0a"
      },
      "execution_count": 11,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}